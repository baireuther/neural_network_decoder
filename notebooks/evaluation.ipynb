{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation notebook\n",
    "\n",
    "This file is part of a decoder for small topological surface and color codes, and potentially other stabilizer codes, when encoding a single logical qubit. The decoder is based on a combination of recurrent and feedforward neural networks.\n",
    "\n",
    "Copyright (c) 2018, Paul Baireuther<br>\n",
    "All rights reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters to be set by user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # FILES AND DIRECTORIES # # #\n",
    "\n",
    "# Set path to source files of this project\n",
    "src_path =  # \"../src/\"\n",
    "\n",
    "# Set path to the checkpoints of the trained decoder\n",
    "checkpoint_path =  # \"../checkpoints/dist3/\"\n",
    "\n",
    "# Set path to the databases that contain the datasets\n",
    "data_path =  # \"../data/color_666_dist_3/\"\n",
    "\n",
    "# List of tuples with test database filenames and corresponding physical error rates\n",
    "fnames = # [(\"colorcode_distance_3_test_p_0.002__canonical.db\", 0.002),\n",
    "#           (\"colorcode_distance_3_test_p_0.003__canonical.db\", 0.003),\n",
    "#           (\"colorcode_distance_3_test_p_0.004__canonical.db\", 0.004),\n",
    "#           (\"colorcode_distance_3_test_p_0.006__canonical.db\", 0.006),\n",
    "#           (\"colorcode_distance_3_test_p_0.010__canonical.db\", 0.010),\n",
    "#           (\"colorcode_distance_3_test_p_0.016__canonical.db\", 0.016),\n",
    "#           (\"colorcode_distance_3_test_p_0.025__canonical.db\", 0.025),\n",
    "#           (\"colorcode_distance_3_test_p_0.040__canonical.db\", 0.040),\n",
    "#           (\"colorcode_distance_3_test_p_0.063__canonical.db\", 0.063),\n",
    "#           (\"colorcode_distance_3_test_p_0.100__canonical.db\", 0.100),\n",
    "#           (\"colorcode_distance_3_test_p_0.160__canonical.db\", 0.160),\n",
    "#           (\"colorcode_distance_3_test_p_0.250__canonical.db\", 0.250)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # PARAMETERS # # #\n",
    "\n",
    "# Network (parameters for 6-6-6 color code with distance 3)\n",
    "code_distance = 3  # distance of the QEC code\n",
    "dim_syndr = 12  # dimension of the syndrome increment vectors\n",
    "dim_fsyndr = 3  # dimension of the final syndrome increment vectors\n",
    "network_size = 32  # size of the LSTM's internal states and number of the the FF layers' neurons\n",
    "\n",
    "# Network (parameters for 6-6-6 color code with distance 5)\n",
    "# code_distance = 5  # distance of the QEC code\n",
    "# dim_syndr = 36  # dimension of the syndrome increment vectors\n",
    "# dim_fsyndr = 9  # dimension of the final syndrome increment vectors\n",
    "# network_size = 64  # size of the LSTM's internal states and number of the the FF layers' neurons\n",
    "\n",
    "# Network (parameters for 6-6-6 color code with distance 7)\n",
    "# code_distance = 7  # distance of the QEC code\n",
    "# dim_syndr = 72  # dimension of the syndrome increment vectors\n",
    "# dim_fsyndr = 18  # dimension of the final syndrome increment vectors\n",
    "# network_size = 128  # size of the LSTM's internal states and number of the the FF layers' neurons\n",
    "\n",
    "# Specify max length of sequences in test datasets\n",
    "max_len_test_sequences = 10000\n",
    "\n",
    "# Evaluation parameters\n",
    "# The number of datapoints per database is given by N = n_batches * batch_size\n",
    "# In order to reduce memory costs at the expense of potentially slower evaluation\n",
    "# decrease batch_size while keeping N constant\n",
    "n_batches = 40  # number of batches ,\n",
    "batch_size = 25  # number of sequences per batch\n",
    "\n",
    "# Bootstrapping to get error estimates\n",
    "bootstrap = True  # if True, error bars are calculated using bootstrapping\n",
    "n_sampling = 5000  # number of draws during bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main part of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # LIBRARIES # # #\n",
    "\n",
    "# Third party libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# This project\n",
    "sys.path.insert(0, src_path);\n",
    "import decoder as dec\n",
    "import database_io as qec_db\n",
    "import qec_functions as fcts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we determine the best instance of the decoder from checkpoint_path/plog_history.dat\n",
    "\n",
    "# Check if the checkpoint_path and contains a model folder exists\n",
    "assert(os.stat(checkpoint_path + \"model/\"))\n",
    "\n",
    "# Determine best model from plog_history.dat\n",
    "plog_history = np.loadtxt(checkpoint_path + \"plog_history.dat\").reshape([-1, 2])\n",
    "best_model_index = int(sorted(plog_history, key=lambda x: x[1])[0][0])\n",
    "print(\"The best model was after epoch\", best_model_index, \n",
    "      \"with logical error rate\", sorted(plog_history, key=lambda x: x[1])[0][1],\n",
    "      \"on the validation dataset.\")\n",
    "\n",
    "# Check if model file exists\n",
    "assert(os.stat(checkpoint_path + \"model/model-\" + str(best_model_index) + \".index\"))\n",
    "assert(os.stat(checkpoint_path + \"model/model-\" + str(best_model_index) + \".meta\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize decoder and load the chosen model\n",
    "net = dec.Decoder(code_distance=code_distance,\n",
    "                  dim_syndr=dim_syndr,\n",
    "                  dim_fsyndr=dim_fsyndr,\n",
    "                  lstm_iss=[network_size, network_size],\n",
    "                  ff_layer_sizes=[network_size],\n",
    "                  checkpoint_path=checkpoint_path,\n",
    "                  keep_prob=1.0,\n",
    "                  aux_loss_factor=0.5,\n",
    "                  l2_prefactor=10**(-5))\n",
    "\n",
    "# Start TensorFlow session\n",
    "net.start_session()\n",
    "\n",
    "# Load the chosen model\n",
    "net.load_network(\"model-\"+str(best_model_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all test databases in the list\n",
    "for fn, p_phys in fnames[::-1]:\n",
    "\n",
    "    # First we load the test database\n",
    "    test_db_fname = data_path + fn\n",
    "    db = qec_db.Data(training_fname=None,\n",
    "                     validation_fname=None,\n",
    "                     test_fname=test_db_fname)\n",
    "    c = db.db_dict[\"test\"].cursor()\n",
    "    print(\"physical error rate in percent:\", p_phys * 100)\n",
    "    \n",
    "    # We then take a single datapoint to learn about the maximum number\n",
    "    # of steps in the test database\n",
    "    test_batches = db.gen_batches(n_batches=1,\n",
    "                                  batch_size=1,\n",
    "                                  db_type=\"test\",\n",
    "                                  len_min=1,\n",
    "                                  len_max=max_len_test_sequences,\n",
    "                                  len_buffered=None)\n",
    "    batch = next(test_batches)\n",
    "    max_len = len(batch[1][0])\n",
    "    \n",
    "    # Based on this we choose a step size\n",
    "    if max_len > 25:\n",
    "        step = int(max_len / 25)\n",
    "    else:\n",
    "        step = 1\n",
    "    step_l = [el for el in list(range(step, max_len + step, step)) if el <= max_len]\n",
    "    \n",
    "    # And then load all the data according to the chosen steps\n",
    "    test_batches = db.gen_batches(n_batches=n_batches,\n",
    "                                  batch_size=batch_size, \n",
    "                                  db_type=\"test\",\n",
    "                                  step_list = step_l,\n",
    "                                  len_min=1,\n",
    "                                  len_max=max_len,\n",
    "                                  len_buffered=None)\n",
    "    \n",
    "    # Now we evaluate the decoder on the test database and calculate\n",
    "    # logical error rate and error bars (optional)\n",
    "    ret = net.test_net(test_batches)\n",
    "    res_dict = fcts.calc_stats(data=ret, \n",
    "                               bootstrap=bootstrap, \n",
    "                               n_sampling=n_sampling, \n",
    "                               p0=(0.001, 0.1),\n",
    "                               lower_bounds=(-0.1, -9999),\n",
    "                               upper_bounds=(0.4, 9999),\n",
    "                               verbose=True)\n",
    "    res_dict['phys'] = p_phys\n",
    "    \n",
    "    # Finally, we plot the fidelity curve ...\n",
    "    fcts.plot_fids(stats_dict=res_dict,\n",
    "                   fmin=min(res_dict['fids']),\n",
    "                   error_bars=bootstrap)\n",
    "    \n",
    "    # ... and print the logical error rate\n",
    "    print(\"logical error rate in percent\", res_dict['plog']*100)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:paul_tf1p8]",
   "language": "python",
   "name": "conda-env-paul_tf1p8-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
