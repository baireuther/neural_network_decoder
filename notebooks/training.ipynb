{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training notebook\n",
    "\n",
    "This file is part of a decoder for small stabilizer codes, in particular color and surface codes, based on a combination of recurrent and feedforward neural networks.\n",
    "\n",
    "Copyright 2018 Paul Baireuther. All Rights Reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters to be set by user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # FILES AND DIRECTORIES # # #\n",
    "\n",
    "# Set path to source files of this project\n",
    "src_path =  # \"../src/\"\n",
    "\n",
    "# Set path where checkpoints, models, and feedback will be stored\n",
    "# WARNING: Existing files in this directory may be overwritten\n",
    "checkpoint_path =  # \"../checkpoints/dist3/\"\n",
    "\n",
    "# Set path to the databases that contain the datasets\n",
    "data_path =  # \"../data/color_666_dist_3/\"\n",
    "\n",
    "# Set path and filenames of databases for training and validation\n",
    "training_db_fname =  # data_path + \"colorcode_distance_3_train_p_0.100__canonical.db\"\n",
    "validation_db_fname =  # data_path + \"colorcode_distance_3_validation_p_0.010__canonical.db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # PARAMETERS # # #\n",
    "\n",
    "# Network (parameters for 6-6-6 color code with distance 3)\n",
    "code_distance = 3  # distance of the QEC code\n",
    "dim_syndr = 12  # dimension of the syndrome increment vectors\n",
    "dim_fsyndr = 3  # dimension of the final syndrome increment vectors\n",
    "network_size = 32  # size of the LSTM's internal states and number of the the FF layers' neurons\n",
    "\n",
    "# Network (parameters for 6-6-6 color code with distance 5)\n",
    "# code_distance = 5  # distance of the QEC code\n",
    "# dim_syndr = 36  # dimension of the syndrome increment vectors\n",
    "# dim_fsyndr = 9  # dimension of the final syndrome increment vectors\n",
    "# network_size = 64  # size of the LSTM's internal states and number of the the FF layers' neurons\n",
    "\n",
    "# Network (parameters for 6-6-6 color code with distance 7)\n",
    "# code_distance = 7  # distance of the QEC code\n",
    "# dim_syndr = 72  # dimension of the syndrome increment vectors\n",
    "# dim_fsyndr = 18  # dimension of the final syndrome increment vectors\n",
    "# network_size = 128  # size of the LSTM's internal states and number of the the FF layers' neurons\n",
    "\n",
    "\n",
    "# Training (example parameters)\n",
    "lr = 0.001  # learning rate\n",
    "keep_prob = 0.8  # keep probability during dropout\n",
    "aux_loss_factor = 0.5  # weight of auxiliary loss term\n",
    "l2_prefactor = 10**(-5)  # prefactor for L2 regularization of weights\n",
    "\n",
    "# Specify the maximum number of stabilizer measurement cycles in training and validation datasets\n",
    "max_len_train_sequences = 40  # the maximum number of cycles in the training dataset\n",
    "max_len_validation_sequences = 10000  # the maximum number cycles in the validation dataset\n",
    "\n",
    "# Other hyperparameters\n",
    "batch_size_training = 64  # batch-size for training\n",
    "no_batches_feedback = 10  # number of batches for feedback\n",
    "batch_size_feedback = 100  # batch-size for feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # TRAINING PIPELINE # # #\n",
    "\n",
    "# Parameters are as follows:\n",
    "# 1. minimum number of stabilizer measurement cycles of training sequences (None means all sequences in dataset can be used)\n",
    "# 2. maximum number of stabilizer measurement cycles of training sequences (None means all sequences in dataset can be used)\n",
    "# 3. number of epochs\n",
    "# 4. batches per epoch\n",
    "train_pipeline = []\n",
    "train_pipeline.append((1,  5, 10, 3000))\n",
    "train_pipeline.append((1, 10, 20, 5000))\n",
    "train_pipeline.append((1, 20, 30, 5000))\n",
    "train_pipeline.append((1, 30, 40, 5000))\n",
    "train_pipeline.append((None, None, 900, 5000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main part of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # LIBRARIES # # #\n",
    "\n",
    "# Third party libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# This project\n",
    "sys.path.insert(0, src_path);\n",
    "import decoder as dec\n",
    "import database_io as qec_db\n",
    "import qec_functions as fcts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # CHECKPOINTS # # #\n",
    "\n",
    "# The training algorithm stores intermediate and final checkpoints \n",
    "# of the decoder during training, as well as feedback                               \n",
    "print(\"cp path is\", checkpoint_path)\n",
    "\n",
    "# Check if the checkpoint_path exists\n",
    "assert(os.stat(checkpoint_path))\n",
    "\n",
    "# Check if subfolder \"model\" already exists and create it if it does not exist\n",
    "try:\n",
    "    os.stat(checkpoint_path + \"model\")\n",
    "except:\n",
    "    os.mkdir(checkpoint_path + \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # DATABASES  # # #\n",
    "\n",
    "# Check if data_path exists\n",
    "assert(os.stat(data_path))\n",
    "\n",
    "# Load databases\n",
    "db = qec_db.Data(training_fname=training_db_fname,\n",
    "                 validation_fname=validation_db_fname,\n",
    "                 test_fname=None,\n",
    "                 verbose=True,\n",
    "                 store_in_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize decoder\n",
    "net = dec.Decoder(code_distance=code_distance,\n",
    "                  dim_syndr=dim_syndr,\n",
    "                  dim_fsyndr=dim_fsyndr,\n",
    "                  lstm_iss=[network_size, network_size],\n",
    "                  ff_layer_sizes=[network_size],\n",
    "                  checkpoint_path=checkpoint_path,\n",
    "                  keep_prob=keep_prob,\n",
    "                  aux_loss_factor=aux_loss_factor,\n",
    "                  l2_prefactor=l2_prefactor)\n",
    "\n",
    "# Start TensorFlow session\n",
    "net.start_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # # Execute training pipeline # # #\n",
    "\n",
    "# In case the training was only stopped, keep using the current best logical error rate plog_best\n",
    "try:\n",
    "    print(\"currently the best logical error rate was\", plog_best)\n",
    "except:\n",
    "    plog_best = 5.0\n",
    "\n",
    "for episode in train_pipeline:\n",
    "    lmin_train, lmax_train, n_epochs, training_batches = episode\n",
    "    \n",
    "    for _ in range(n_epochs):\n",
    "        epoch_idx = net.total_trained_epochs + 1\n",
    "        print(\"epoch #\" + str(epoch_idx))\n",
    "        \n",
    "        # Train for one epoch\n",
    "        train_batches = db.gen_batches(n_batches=training_batches,\n",
    "                                       batch_size=batch_size_training,\n",
    "                                       db_type=\"training\",\n",
    "                                       len_buffered=max_len_train_sequences,\n",
    "                                       len_min=lmin_train,\n",
    "                                       len_max=lmax_train,\n",
    "                                       select_random=True)\n",
    "        net.train_one_epoch(train_batches=train_batches,\n",
    "                            learning_rate=lr)\n",
    "        \n",
    "        # Generate feedback using training dataset\n",
    "        train_batches = db.gen_batches(n_batches=no_batches_feedback,\n",
    "                                       batch_size=batch_size_feedback,\n",
    "                                       db_type=\"training\",\n",
    "                                       len_buffered=max_len_train_sequences,\n",
    "                                       len_min=lmin_train,\n",
    "                                       len_max=lmax_train,\n",
    "                                       select_random=False)\n",
    "        plog_train = net.calc_feedback(batches=train_batches,\n",
    "                                       validation=False)\n",
    "        \n",
    "        # Generate feedback using validation dataset\n",
    "        validation_batches = db.gen_batches(n_batches=no_batches_feedback, \n",
    "                                            batch_size=batch_size_feedback,\n",
    "                                            db_type=\"validation\",\n",
    "                                            len_buffered=max_len_validation_sequences,\n",
    "                                            len_min=None,\n",
    "                                            len_max=None, \n",
    "                                            select_random=False)\n",
    "        plog_val = net.calc_feedback(batches=validation_batches,\n",
    "                                     validation=True)\n",
    "        \n",
    "        # Check if there is a new record logical error rate on the validation dataset\n",
    "        if plog_val < plog_best:\n",
    "            plog_best = plog_val\n",
    "            print(\"--> new record logical error rate is\", round(plog_best, 4))\n",
    "            net.save_network(\"best\")\n",
    "        \n",
    "        # Save history of logical error rates on validation dataset\n",
    "        try:\n",
    "            plog_history = np.loadtxt(net.cp_path + \"plog_history.dat\").reshape([-1, 2])\n",
    "            plog_history = np.concatenate((plog_history, np.array([[epoch_idx, plog_val]])), axis=0)\n",
    "        except:\n",
    "            plog_history = np.array([[epoch_idx, plog_val]])\n",
    "        np.savetxt(net.cp_path + \"plog_history.dat\", plog_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:paul_tf1p8]",
   "language": "python",
   "name": "conda-env-paul_tf1p8-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
